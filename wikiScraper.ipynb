{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "\n",
    "   \"Health\":[\n",
    "      \"Medicine\",\n",
    "      \"Nutrition\",\n",
    "      \"Mental Health Trends\",\n",
    "      \"Epidemiology\",\n",
    "      \"Vaccination\",\n",
    "      \"Health Insurance\",\n",
    "      \"Genetic Disorders\",\n",
    "      \"Pharmaceutical Industry\",\n",
    "      \"Global Health Organizations\",\n",
    "      \"Medical Devices\",\n",
    "      \"Chronic Illnesses\",\n",
    "      \"Addiction Medicine\",\n",
    "      \"Sleep Disorders\"\n",
    "   ],\n",
    "   \"Environment\":[\n",
    "      \"Climate Change\",\n",
    "      \"Pollution\",\n",
    "      \"Recycling\",\n",
    "      \"Deforestation\",\n",
    "      \"Endangered Species\",\n",
    "      \"Natural Disasters\",\n",
    "      \"Sustainable Agriculture\",\n",
    "      \"Renewable Energy\",\n",
    "      \"Ecotourism\",\n",
    "      \"Oceans and Marine Life\",\n",
    "      \"Conservation Biology\",\n",
    "      \"Carbon Footprint\",\n",
    "      \"Volcanoes\",\n",
    "   ],\n",
    "   \"Technology\":[\n",
    "      \"Artificial Intelligence\",\n",
    "      \"Machine Learning\",\n",
    "      \"Cybersecurity\",\n",
    "      \"Quantum Computing\",\n",
    "      \"5G\",\n",
    "      \"Blockchain\",\n",
    "      \"Augmented Reality\",\n",
    "      \"Internet of Things\",\n",
    "      \"Biotechnology\",\n",
    "      \"Nanotechnology\",\n",
    "      \"Space Exploration Technologies\",\n",
    "      \"Computer Vision\",\n",
    "      \"Human-Computer Interaction\"\n",
    "   ],\n",
    "   \"Economy\":[\n",
    "      \"Stock Market\",\n",
    "      \"Inflation\",\n",
    "      \"Unemployment\",\n",
    "      \"GDP\",\n",
    "      \"Consumer Price Index\",\n",
    "      \"Interest Rates\",\n",
    "      \"Microeconomics\",\n",
    "      \"Macroeconomics\",\n",
    "      \"Income Inequality\",\n",
    "      \"Housing Market\",\n",
    "      \"Fiscal Policy\",\n",
    "      \"Venture Capital\",\n",
    "      \"Labor Market Trends\"\n",
    "   ],\n",
    "   \"Entertainment\":[\n",
    "      \"Music\",\n",
    "      \"E Sports\",\n",
    "      \"Video Games\",\n",
    "      \"Artists\",\n",
    "      \"Youtube\",\n",
    "      \"Streaming Services\",\n",
    "      \"Netflix\",\n",
    "      \"Film Festivals\",\n",
    "      \"Celebrity Culture\",\n",
    "      \"Pop Culture\",\n",
    "      \"Theater and Performing Arts\",\n",
    "      \"Stand-up Comedy\",\n",
    "      \"Animation\",\n",
    "      \"Reality TV\",\n",
    "      \"Anime\",\n",
    "      \"Virtual Reality\",\n",
    "   ],\n",
    "   \"Sports\":[\n",
    "      \"Football\",\n",
    "      \"Basketball\",\n",
    "      \"Soccer\",\n",
    "      \"Baseball\",\n",
    "      \"Hockey\",\n",
    "      \"Tennis\",\n",
    "      \"Golf\",\n",
    "      \"Sports Events\",\n",
    "      \"Olympics\",\n",
    "      \"Martial Arts\",\n",
    "      \"Winter Sports\",\n",
    "      \"Extreme Sports\",\n",
    "      \"NFL\",\n",
    "      \"NBA\",\n",
    "   ],\n",
    "   \"Politics\":[\n",
    "      \"Election\",\n",
    "      \"Public Policy\",\n",
    "      \"Political Parties\",\n",
    "      \"Government\",\n",
    "      \"Political Leaders\",\n",
    "      \"Political Movements\",\n",
    "      \"International Relations\",\n",
    "      \"Constitutional Law\",\n",
    "      \"Political Ideologies\",\n",
    "      \"Diplomacy\",\n",
    "      \"Human Rights\",\n",
    "      \"Global Governance\",\n",
    "      \"National Security\",\n",
    "      \"Political Theories\",\n",
    "      \"Voting Systems\",\n",
    "      \"Civic Participation\",\n",
    "      \"Policy Analysis\",\n",
    "      \"Civil Liberties\",\n",
    "      \"Geopolitical Conflicts\",\n",
    "      \"Regional Alliances\"\n",
    "   ],\n",
    "   \"Education\":[\n",
    "      \"Literacy Rate\",\n",
    "      \"Masters Degree\",\n",
    "      \"Universities\",\n",
    "      \"Online Learning\",\n",
    "      \"Education Statistics\",\n",
    "      \"University at Buffalo\",\n",
    "      \"Women In STEM\",\n",
    "      \"Philosophy\",\n",
    "      \"Physics\",\n",
    "      \"Mathematics\",\n",
    "   ],\n",
    "   \"Travel\":[\n",
    "      \"Tourists\",\n",
    "      \"Airline Industry\",\n",
    "      \"Railway\",\n",
    "      \"Cruise Ships\",\n",
    "      \"Travel Insurance\",\n",
    "      \"Backpacking Culture\",\n",
    "      \"Travel Blogs\",\n",
    "      \"Adventure Tourism\",\n",
    "      \"Travel Photography\",\n",
    "      \"National Parks\",\n",
    "      \"Visa Policies\",\n",
    "      \"International Travel Regulations\",\n",
    "   ],\n",
    "   \"Food\":[\n",
    "      \"Indian Food\",\n",
    "      \"Spices\",\n",
    "      \"Street Food\",\n",
    "      \"Fast Food\",\n",
    "      \"Nutrition Science\", \n",
    "      \"Dietary Supplements\", \n",
    "      \"Food Preservation\", \n",
    "      \"Ethnic Cuisines\", \n",
    "      \"Baking and Pastry\",\n",
    "      \"Food Allergies\", \n",
    "      \"Healthy Eating\", \n",
    "      \"Fermentation Techniques\", \n",
    "      \"Food and Culture\"\n",
    "    \n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohan/UB/sem2/IR/irVenv/lib/python3.12/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/rohan/UB/sem2/IR/irVenv/lib/python3.12/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraping.log', level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s %(threadName)s')\n",
    "\n",
    "\n",
    "def getTopicPages(searchQueries: list, topicName: str, nPages=1200) -> list:\n",
    "    pages = []\n",
    "    resultsPerSubtopic = max(1, nPages // len(searchQueries))\n",
    "    uniquePagesUrls = set()\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def processSearchResult(searchResult):\n",
    "        pageData = {}\n",
    "        disambiguationErrorCount = 0\n",
    "        retry_attempts = 0\n",
    "        max_retries = 4\n",
    "\n",
    "        while retry_attempts < max_retries:\n",
    "            try:\n",
    "                page = wikipedia.page(searchResult, auto_suggest=False, preload=True)\n",
    "                time.sleep(0.25)\n",
    "                if len(page.summary) < 200:\n",
    "                    logging.info(f\"Summary too short for {searchResult}\")\n",
    "                    return None\n",
    "                with lock:\n",
    "                    if page.url in uniquePagesUrls:\n",
    "                        logging.info(f\"Page already added: {page.title}\")\n",
    "                        return None\n",
    "                    uniquePagesUrls.add(page.url)\n",
    "                pageData[\"revision_id\"] = page.revision_id\n",
    "                pageData[\"title\"] = page.title\n",
    "                pageData[\"url\"] = page.url\n",
    "                pageData[\"summary\"] = page.summary\n",
    "                pageData[\"topic\"] = topicName\n",
    "                pageData[\"content\"] = page.content\n",
    "                logging.info(f\"Page added: {pageData['title']}\")\n",
    "                return pageData\n",
    "\n",
    "            except (TimeoutError, ConnectionError, OSError) as e:\n",
    "                logging.warning(f\"TimeoutError on Page: {searchResult}, Error: {e}\")\n",
    "                time.sleep(2 ** retry_attempts)\n",
    "                retry_attempts += 1\n",
    "\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                disambiguationErrorCount += 1\n",
    "                logging.warning(f\"DisambiguationError on Page: {searchResult}, Options: {e.options}\")\n",
    "                if disambiguationErrorCount > 3:\n",
    "                    logging.warning(f\"DisambiguationError count exceeded 3 for {searchResult}\")\n",
    "                    return None\n",
    "                searchResult = e.options[0]\n",
    "\n",
    "            except wikipedia.exceptions.PageError as e:\n",
    "                # logging.error(f\"PageError on Page: {searchResult}, Error: {e}\")\n",
    "                return None\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error on Page: {searchResult}, Error: {e}\")\n",
    "                return None\n",
    "\n",
    "        logging.error(f\"Failed to process {searchResult} after {max_retries} retries\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "            futures = []\n",
    "            for searchQuery in searchQueries:\n",
    "                try:\n",
    "                    searchResults = wikipedia.search(searchQuery, results=resultsPerSubtopic)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Exception during search for query '{searchQuery}' in topic '{topicName}': {e}\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"{len(searchResults)} results found for '{searchQuery}' in topic '{topicName}'\")\n",
    "                searchResults = list(set(searchResults))\n",
    "\n",
    "                for searchResult in searchResults:\n",
    "                    with lock:\n",
    "                        if len(pages) >= nPages:\n",
    "                            break\n",
    "                    future = executor.submit(processSearchResult, searchResult)\n",
    "                    futures.append(future)\n",
    "\n",
    "                with lock:\n",
    "                    if len(pages) >= nPages:\n",
    "                        break\n",
    "\n",
    "            # Collect results as they complete\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result is not None:\n",
    "                        with lock:\n",
    "                            if len(pages) >= nPages:\n",
    "                                break\n",
    "                            pages.append(result)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Exception in processing future in topic '{topicName}': {e}\")\n",
    "\n",
    "                with lock:\n",
    "                    if len(pages) >= nPages:\n",
    "                        break\n",
    "\n",
    "        logging.info(f\"Total pages collected for topic '{topicName}': {len(pages)}\")\n",
    "        return pages\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception in getTopicPages for topic '{topicName}': {e}\")\n",
    "        return []\n",
    "\n",
    "def scrapeAndSave():\n",
    "    dataDict = dict()\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        future_to_topic = {executor.submit(getTopicPages, subtopics, topicName): topicName for topicName, subtopics in topics.items()}\n",
    "        for future in as_completed(future_to_topic):\n",
    "            topicName = future_to_topic[future]\n",
    "            try:\n",
    "                pages = future.result()\n",
    "                if pages:\n",
    "                    dataDict[topicName] = pages\n",
    "                    logging.info(f\"Collected {len(pages)} pages for topic '{topicName}'\")\n",
    "                else:\n",
    "                    logging.warning(f\"No pages collected for topic '{topicName}'\")\n",
    "            except Exception as exc:\n",
    "                logging.error(f\"Exception in scrapeAndSave for topic '{topicName}': {exc}\")\n",
    "\n",
    "    with open(\"data5.json\", \"w\") as f:\n",
    "        json.dump(dataDict, f, indent=4)\n",
    " \n",
    "    return dataDict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrapeAndSave()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages Unique for  Economy  :  5305\n",
      "Number of pages Unique for  Health  :  5649\n",
      "Number of pages Unique for  Environment  :  5277\n",
      "Number of pages Unique for  Technology  :  5559\n",
      "Number of pages Unique for  Entertainment  :  5764\n",
      "Number of pages Unique for  Sports  :  5844\n",
      "Number of pages Unique for  Travel  :  5041\n",
      "Number of pages Unique for  Education  :  5325\n",
      "Number of pages Unique for  Politics  :  5264\n",
      "Number of pages Unique for  Food  :  5209\n",
      "Number of pages with summary less than 200 characters for  Economy  :  0\n",
      "Number of pages with summary less than 200 characters for  Health  :  0\n",
      "Number of pages with summary less than 200 characters for  Environment  :  0\n",
      "Number of pages with summary less than 200 characters for  Technology  :  0\n",
      "Number of pages with summary less than 200 characters for  Entertainment  :  0\n",
      "Number of pages with summary less than 200 characters for  Sports  :  0\n",
      "Number of pages with summary less than 200 characters for  Travel  :  0\n",
      "Number of pages with summary less than 200 characters for  Education  :  0\n",
      "Number of pages with summary less than 200 characters for  Politics  :  0\n",
      "Number of pages with summary less than 200 characters for  Food  :  0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# load data\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "# Remove duplicates from the data based on url\n",
    "urls = set()\n",
    "for topic in data.keys():\n",
    "    uniquePages = []\n",
    "    \n",
    "    for page in data[topic]:\n",
    "        if page[\"url\"] not in urls and len(page[\"summary\"]) > 200:\n",
    "            uniquePages.append(page)\n",
    "            urls.add(page[\"url\"])\n",
    "    data[topic] = uniquePages\n",
    "\n",
    "# Count the number of pages per topic\n",
    "for topic in data.keys():\n",
    "    print(\"Number of pages Unique for \", topic, \" : \", len(data[topic]))\n",
    "\n",
    "\n",
    "# Check for number of pages with summary less than 200 characters for each topic\n",
    "\n",
    "for topic in data.keys():\n",
    "    count = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
